{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterthemes import jtplot\n",
    "# onedork | grade3 | oceans16 | chesterish | monokai | solarizedl | solarizedd\n",
    "jtplot.style(theme='grade3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMEN NLP :\n",
    "### Sujet : classifieur de commentaire produit.\n",
    "#### Produit : Les cinq blessures qui empêchent d'être soi-même.\n",
    "#### lien : https://www.amazon.fr/cinq-blessures-emp%C3%AAchent-d%C3%AAtre-soi-m%C3%AAme/dp/2266229486/ref=cm_cr_arp_d_product_top?ie=UTF8\n",
    "\n",
    "#### Année : 2020 - 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Présenté par :\n",
    "| Prénoms       |     Nom         |   \n",
    "| ------------- |: -------------: |\n",
    "| Aboubacar Sidiki        |        SIDIBE        |\n",
    "\n",
    "GROUPE GEMA / IA-SCHOOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 3: Classifier et performance des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chargement de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('comment_clean.csv',encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commentaire</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>utilisation developpement personnel excellent ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>livrer lire relire mieux decouvrir auteur form...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apprendre changer choses comprendre tolerance ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rechercher livrer interessant recommander comp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>parfait adorer recommander remettre question r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         commentaire  label\n",
       "0  utilisation developpement personnel excellent ...      1\n",
       "1  livrer lire relire mieux decouvrir auteur form...      1\n",
       "2  apprendre changer choses comprendre tolerance ...      1\n",
       "3  rechercher livrer interessant recommander comp...      1\n",
       "4  parfait adorer recommander remettre question r...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split des données en train data et test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reviews = df['commentaire']\n",
    "labels = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, random_state = 42, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 2056)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_final = vectorizer.fit_transform(X_train)\n",
    "\n",
    "print(X_train_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametrage et entrainement du mdoèle avec pipeline scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),('clf', LinearSVC()),])\n",
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid_predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7533333333333333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construction de la liste des phrases à partir du corpus avec le X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "list_sents = [nltk.word_tokenize(sent) for sent_tok in X_train for sent in nltk.sent_tokenize(sent_tok)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parametrge du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les paramètre choisis pour le parametrage de notre modèle\n",
    "\n",
    "- min_count = int - Ignore tous les mots dont la fréquence absolue totale est inférieure à cette valeur - (2, 100)\n",
    "\n",
    "- window = int - La distance maximale entre le mot actuel et le mot prédit dans une phrase. Par exemple, les mots fenêtre à gauche et les mots fenêtre à gauche de notre cible - (2, 10)\n",
    "\n",
    "- vector_size = int - Dimensionnalité des vecteurs de caractéristiques. - (50, 300)\n",
    "\n",
    "- sample = float - Le seuil pour configurer quels mots à haute fréquence sont aléatoirement sous-échantillonnés. Très influent. - (0, 1e-5)\n",
    "\n",
    "- alpha = float - Le taux d'apprentissage initial - (0.01, 0.05)\n",
    "\n",
    "- min_alpha = float - Le taux d'apprentissage diminuera linéairement jusqu'à min_alpha au fur et à mesure de l'apprentissage. Pour le définir : alpha - (min_alpha * epochs) ~ 0.00\n",
    "\n",
    "- negative = int - Si > 0, un échantillonnage négatif sera utilisé, l'int pour negative spécifie combien de \"mots de bruit\" doivent être noyés. Si la valeur est 0, aucun échantillonnage négatif n'est utilisé. - (5, 20)\n",
    "\n",
    "- workers = int - Utilisez ce nombre de threads pour entraîner le modèle (=entraînement plus rapide avec des machines multicœurs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIDIKI\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(sentences=list_sents, \n",
    "                          min_count=20,\n",
    "                          window=12,\n",
    "                          vector_size=80,\n",
    "                          sample=6e-5, \n",
    "                          alpha=0.03, \n",
    "                          min_alpha=0.0007, \n",
    "                          negative=20,\n",
    "                          workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.corpus_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construction du tableau de vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tmps1=time.time()\n",
    "\n",
    "w2v_model.build_vocab(list_sents, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time.time()-tmps1) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramètres de l'entraînement :\n",
    "\n",
    "- total_examples = int - Nombre de phrases ;\n",
    "- epochs = int - Nombre d'itérations (epochs) sur le corpus - [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "w2v_model.train(list_sents, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nous appelons init_sims(), ce qui rendra le modèle beaucoup plus efficace en termes de mémoire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-c7757d71a30b>:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v_model.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "### function to get vector\n",
    "def get_vect(word, model):\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except KeyError:\n",
    "        return numpy.zeros((model.vector_size,))\n",
    "\n",
    "### function to sum vector\n",
    "def sum_vectors(phrase, model):\n",
    "    return sum(get_vect(w, model) for w in phrase)\n",
    "\n",
    "### function to get feature\n",
    "def word2vec_features(X, model):\n",
    "    feats = numpy.vstack([sum_vectors(p, model) for p in X])\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 80)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_train_feat = word2vec_features(X_train, w2v_model)\n",
    "wv_train_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement d'un modèle logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clfwv = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "clfwv.fit(wv_train_feat, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### featurisation du data\n",
    "wv_test_feat = word2vec_features(X_test, w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prediction\n",
    "w2v_predictions = clfwv.predict(wv_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49333333333333335"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### score\n",
    "clfwv.score(wv_test_feat, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. CamenBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous commençons par installer transformers et sentencepiece, deux packages nécessaires à l’usage de Camembert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model, tokenizer and weights\n",
    "camembert, tokenizer, weights = (ppb.CamembertModel, ppb.CamembertTokenizer, 'camembert-base')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer.from_pretrained(weights)\n",
    "model = camembert.from_pretrained(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on enleve les commentaires trop longs. Car Bert ne sait que tokéniser des phrases de longueur maximale de 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  233\n"
     ]
    }
   ],
   "source": [
    "# see if there are length > 512\n",
    "max_len = 0\n",
    "for i,sent in enumerate(reviews):\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    if len(input_ids) > 512:\n",
    "        print(\"annoying review at\", i,\"with length\",\n",
    "              len(input_ids))\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pas de phrases trouvées dont la longeur depasse la longueur max autorisée (512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nous faisons un padding de 472 tokens pour homogénéiser la longueur de phrases. Cela rendra l’entraînement plus simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 233)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = reviews.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 233)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encodage du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nous transformons les tokens en tensor pour les passer dans le transformer. \n",
    "- seule la dernière couche est conservée pour faire la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement d'un modèle logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous avons seulement besoin du premier token (CLS qui signifie classification) pour le modèle logistique, nous faisons en slice avec [:,0,:]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "495    0\n",
       "496    0\n",
       "497    0\n",
       "498    0\n",
       "499    0\n",
       "Name: label, Length: 500, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "labels = labels\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split data avec les nouvelles features\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:  {'C': 5.263252631578947}\n",
      "best scrores:  0.768\n"
     ]
    }
   ],
   "source": [
    "# Grid search pour le choix du bon parametrage\n",
    "parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
    "grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scrores: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "configuration du modèle avec les bons paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5.263252631578947)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(C=grid_search.best_params_['C'])\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "camembert_predictions = lr_clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.752"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### score\n",
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyse des performences des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'analyse de performences des modèles, nous sommes basés sur les metrics suivants:\n",
    "- Precision : mesure la précision des prédictions.\n",
    "- Recall : indique le pourcentage des classes qui nous intéressent qui ont été effectivement capturées par le modèle.\n",
    "- Accuracy : est probablement la mesure la plus intuitive. Nous l'utilisons pour mesurer le nombre total de prédictions correctes d'un modèle, y compris les vrais positifs et les vrais négatifs.\n",
    "- F1-score : est une mesure qui prend en compte à la fois la précision et le rappel pour calculer le score. Il peut être interprété comme une moyenne pondérée des valeurs de précision et de rappel et ne peut être élevé sans que la précision et le rappel le soient également. Lorsque le score F1 d'un modèle est élevé, on peut savoir que notre modèle fonctionne bien dans l'ensemble.\n",
    "- Matthews correlation coefficient(MCC) : est utilisé en apprentissage automatique comme mesure de la qualité des classifications binaires et multiclasses. Il prend en compte les vrais et faux positifs et négatifs et est généralement considéré comme une mesure équilibrée qui peut être utilisée même si les classes sont de tailles très différentes.\n",
    "- Matrice de confusion (confusion matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Evaluation de TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7533333333333333\n",
      "Precision: 0.7349397590361446\n",
      "Recall: 0.8026315789473685\n",
      "F1-score: 0.7672955974842768\n",
      "MCC: 0.5081886603621535\n",
      "Confusion matrix: [[52 22]\n",
      " [15 61]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.70      0.74        74\n",
      "           1       0.73      0.80      0.77        76\n",
      "\n",
      "    accuracy                           0.75       150\n",
      "   macro avg       0.76      0.75      0.75       150\n",
      "weighted avg       0.76      0.75      0.75       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, tfid_predictions))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, tfid_predictions))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, tfid_predictions))\n",
    "print(\"F1-score:\", metrics.f1_score(y_test, tfid_predictions))\n",
    "print(\"MCC:\", metrics.matthews_corrcoef(y_test, tfid_predictions))\n",
    "print(\"Confusion matrix:\", metrics.confusion_matrix(y_test, tfid_predictions))\n",
    "print(metrics.classification_report(y_test, tfid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Evaluation de Word2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49333333333333335\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-score: 0.0\n",
      "MCC: 0.0\n",
      "Confusion matrix: [[74  0]\n",
      " [76  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      1.00      0.66        74\n",
      "           1       0.00      0.00      0.00        76\n",
      "\n",
      "    accuracy                           0.49       150\n",
      "   macro avg       0.25      0.50      0.33       150\n",
      "weighted avg       0.24      0.49      0.33       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, w2v_predictions))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, w2v_predictions))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, w2v_predictions))\n",
    "print(\"F1-score:\", metrics.f1_score(y_test, w2v_predictions))\n",
    "print(\"MCC:\", metrics.matthews_corrcoef(y_test, w2v_predictions))\n",
    "print(\"Confusion matrix:\", metrics.confusion_matrix(y_test, w2v_predictions))\n",
    "print(metrics.classification_report(y_test, w2v_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Evaluation de Camembert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.752\n",
      "Precision: 0.7368421052631579\n",
      "Recall: 0.835820895522388\n",
      "F1-score: 0.7832167832167832\n",
      "MCC: 0.5015591250297746\n",
      "Confusion matrix: [[38 20]\n",
      " [11 56]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.66      0.71        58\n",
      "           1       0.74      0.84      0.78        67\n",
      "\n",
      "    accuracy                           0.75       125\n",
      "   macro avg       0.76      0.75      0.75       125\n",
      "weighted avg       0.75      0.75      0.75       125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(test_labels, camembert_predictions))\n",
    "print(\"Precision:\", metrics.precision_score(test_labels, camembert_predictions))\n",
    "print(\"Recall:\", metrics.recall_score(test_labels, camembert_predictions))\n",
    "print(\"F1-score:\", metrics.f1_score(test_labels, camembert_predictions))\n",
    "print(\"MCC:\", metrics.matthews_corrcoef(test_labels, camembert_predictions))\n",
    "print(\"Confusion matrix:\", metrics.confusion_matrix(test_labels, camembert_predictions))\n",
    "print(metrics.classification_report(test_labels, camembert_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tableau récapitulatif des évaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metrics / Modèles       |     TFIDF      | WORD2VEC       |   CAMEMBERT     |   \n",
    "| :------------ | -------------  |  ------------  |  -------------: |\n",
    "|Accuracy       |  0.753         |   0.493        |    0.752       |\n",
    "|Precision      |  0.734         |   0.0          |    0.736       |\n",
    "|Recall         |  0.802         |   0.0          |    0.835       |\n",
    "|F1-score       |  0.767         |   0.0          |    0.783       |\n",
    "|MCC            |   0.508        |   0.0          |    0.501       |\n",
    "|Confusion matrix| [[52 22][15 61]]|  [[74  0][76  0]]|  [[38 20][11 56]]|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On remarque sur le tableau récapitulatif ci-dessus que seul le modèle WORD2VEC a un score beacoup plus faible, cela est dû à plusieurs raisons. La raison de la précision beaucoup trop faiblre est que j'ai équilibré l'ensemble de données, afin d'avoir une distribution égale des classes dans l'ensemble d'entraînement final. Par conséquent, j'ai suréchantillonné la classe avec moins de données. Cela signifie bien sûr qu'ensuite l'ensemble de données contient les données suréchantillonnées plusieurs fois. Si on sépare les données de test après le suréchantillonnage, il est très, très probable que vos données d'apprentissage contiennent des éléments provenant des données suréchantillonnées.\n",
    "\n",
    "- Une des raisons ce score faible est le parametrage.\n",
    "\n",
    "- Par contre, il n'y pas trop de différence entre les scores des modèles TFIDF et Camembert et ils sont élévés. Cela prouve que ces deux modèles fonctionnent bien dans l'ensemble car ils ont tous deux un F1-score élevés.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
